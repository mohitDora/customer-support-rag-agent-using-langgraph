{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de8ea20",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, TypedDict, Literal\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29319a8",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e088e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_DB_DIR = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"customer_support_knowledge\"\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d2661",
   "metadata": {},
   "source": [
    "### Initializing LLM and embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab181b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2, google_api_key=GOOGLE_API_KEY)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938d9e9",
   "metadata": {},
   "source": [
    "### Defining agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91815f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our RAG agent's workflow for a single sub-query.\n",
    "    \"\"\"\n",
    "    current_sub_query: str\n",
    "    retrieved_chunks: List[Document]\n",
    "    evaluated_sufficiency: bool\n",
    "    evaluator_feedback: str\n",
    "    retrieval_attempts: int\n",
    "    next_agent_to_call: Literal['retriever_agent', 'evaluator_agent', 'END_PHASE2', 'END_PHASE2_FAILURE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f65c0",
   "metadata": {},
   "source": [
    "### Retriever Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever_agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Node that retrieves relevant chunks from the vector database based on the current sub-query.\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVER AGENT: Initiating retrieval---\")\n",
    "    current_sub_query = state[\"current_sub_query\"]\n",
    "    retrieval_attempts = state.get(\"retrieval_attempts\", 0) + 1\n",
    "\n",
    "    try:\n",
    "        # Load the existing ChromaDB instance\n",
    "        vector_db = Chroma(\n",
    "            persist_directory=CHROMA_DB_DIR,\n",
    "            embedding_function=embeddings, # MUST use the same embedding function\n",
    "            collection_name=COLLECTION_NAME\n",
    "        )\n",
    "        print(f\"ChromaDB loaded successfully from {CHROMA_DB_DIR}.\")\n",
    "\n",
    "        # Retrieve top K results (e.g., top 3 or 5)\n",
    "        # We can dynamically adjust 'k' or other search_kwargs here if needed\n",
    "        # For now, let's keep it fixed, but evaluator_feedback could refine this.\n",
    "        k_value = 1 # You can experiment with this number\n",
    "        retriever = vector_db.as_retriever(search_kwargs={\"k\": k_value})\n",
    "\n",
    "        print(f\"Retrieving for query: '{current_sub_query}' (Attempt: {retrieval_attempts})\")\n",
    "        retrieved_chunks = retriever.invoke(current_sub_query)\n",
    "\n",
    "        print(f\"---RETRIEVER AGENT: Retrieved {len(retrieved_chunks)} chunks.---\")\n",
    "\n",
    "        return {\n",
    "            \"current_sub_query\": current_sub_query,\n",
    "            \"retrieved_chunks\": retrieved_chunks,\n",
    "            \"retrieval_attempts\": retrieval_attempts,\n",
    "            \"next_agent_to_call\": 'evaluator_agent', # Always go to evaluator after retrieval\n",
    "            \"evaluated_sufficiency\": False, # Reset for next evaluation\n",
    "            \"evaluator_feedback\": \"\" # Reset feedback\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"---RETRIEVER AGENT ERROR: {e}---\")\n",
    "        return {\n",
    "            \"current_sub_query\": current_sub_query,\n",
    "            \"retrieved_chunks\": [],\n",
    "            \"retrieval_attempts\": retrieval_attempts,\n",
    "            \"next_agent_to_call\": 'END_PHASE2_FAILURE', # End if retrieval fails completely\n",
    "            \"evaluated_sufficiency\": False,\n",
    "            \"evaluator_feedback\": f\"Retrieval failed: {e}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3da1a",
   "metadata": {},
   "source": [
    "### Evaluator agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1373938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Node that uses Gemini to evaluate if the retrieved chunks are sufficient to answer the sub-query.\n",
    "    Provides feedback if not.\n",
    "    \"\"\"\n",
    "    print(\"---EVALUATOR AGENT: Evaluating retrieved chunks---\")\n",
    "    current_sub_query = state[\"current_sub_query\"]\n",
    "    retrieved_chunks = state[\"retrieved_chunks\"]\n",
    "    retrieval_attempts = state[\"retrieval_attempts\"]\n",
    "\n",
    "    if retrieval_attempts > 2:\n",
    "        print(\"---EVALUATOR AGENT: Max retrieval attempts reached. Marking as insufficient.---\")\n",
    "        return {\n",
    "            \"current_sub_query\": current_sub_query,\n",
    "            \"retrieved_chunks\": retrieved_chunks,\n",
    "            \"evaluated_sufficiency\": False,\n",
    "            \"evaluator_feedback\": \"Max retrieval attempts reached. Marking as insufficient.\",\n",
    "            \"retrieval_attempts\": retrieval_attempts,\n",
    "            \"next_agent_to_call\": 'END_PHASE2_FAILURE'\n",
    "        }\n",
    "\n",
    "    if not retrieved_chunks:\n",
    "        print(\"---EVALUATOR AGENT: No chunks retrieved, marking as insufficient.---\")\n",
    "        return {\n",
    "            \"current_sub_query\": current_sub_query,\n",
    "            \"retrieved_chunks\": retrieved_chunks,\n",
    "            \"evaluated_sufficiency\": False,\n",
    "            \"evaluator_feedback\": \"No relevant documents were retrieved for this sub-query. Try rephrasing the sub-query or check if the information exists in the knowledge base.\",\n",
    "            \"retrieval_attempts\": retrieval_attempts,\n",
    "            \"next_agent_to_call\": 'retriever_agent' # Try retrieval again, will be capped by max_retries\n",
    "        }\n",
    "\n",
    "    # Format chunks for the LLM prompt\n",
    "    formatted_chunks = \"\\n---\\n\".join([doc.page_content for doc in retrieved_chunks])\n",
    "    \n",
    "    # Define the evaluation prompt for Gemini\n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \n",
    "         \"\"\"You are an expert evaluator for a RAG system. Your task is to determine if the provided 'CONTEXT' is sufficient and relevant to fully and comprehensively answer the 'SUB-QUERY'.\n",
    "\n",
    "         If the CONTEXT is sufficient and relevant, respond ONLY with the word 'YES'.\n",
    "         If the CONTEXT is NOT sufficient or relevant, respond ONLY with the word 'NO', followed by a concise, specific suggestion on how to improve the retrieval for this sub-query. For example:\n",
    "         - 'NO: The results are too general; focus on error codes.'\n",
    "         - 'NO: No specific instructions found for this type of issue.'\n",
    "         - 'NO: The context mentions the topic but lacks actionable steps.'\n",
    "\n",
    "         Aim for direct, actionable feedback. Do not elaborate beyond 'NO: [feedback]'.\n",
    "         \"\"\"),\n",
    "        (\"human\", \n",
    "         f\"SUB-QUERY: {current_sub_query}\\n\\nCONTEXT:\\n{formatted_chunks}\")\n",
    "    ])\n",
    "\n",
    "    eval_chain = eval_prompt | llm | StrOutputParser()\n",
    "\n",
    "    print(f\"---EVALUATOR AGENT: Sending evaluation request to Gemini for query '{current_sub_query}'---\")\n",
    "    gemini_response = eval_chain.invoke({\"current_sub_query\": current_sub_query, \"formatted_chunks\": formatted_chunks})\n",
    "    \n",
    "    gemini_response = gemini_response.strip()\n",
    "    print(f\"---EVALUATOR AGENT: Gemini's raw response: {gemini_response}---\")\n",
    "\n",
    "    is_sufficient = gemini_response.upper().startswith(\"YES\")\n",
    "    feedback = \"\"\n",
    "    if not is_sufficient and len(gemini_response) > 3: # \"NO\" is 2 chars, so look for more\n",
    "        feedback = gemini_response[3:].strip() # Remove \"NO:\" prefix\n",
    "\n",
    "    print(f\"---EVALUATOR AGENT: Sufficiency: {is_sufficient}, Feedback: '{feedback}'---\")\n",
    "\n",
    "    next_step: Literal['retriever_agent', 'evaluator_agent', 'END_PHASE2', 'END_PHASE2_FAILURE']\n",
    "    MAX_RETRIEVAL_ATTEMPTS = 2 # Define maximum attempts\n",
    "\n",
    "    if is_sufficient:\n",
    "        next_step = 'END_PHASE2' # Good enough, proceed to next phase (or final answer generation)\n",
    "    elif retrieval_attempts >= MAX_RETRIEVAL_ATTEMPTS:\n",
    "        print(f\"---EVALUATOR AGENT: Max retrieval attempts ({MAX_RETRIEVAL_ATTEMPTS}) reached. Ending phase for this query.---\")\n",
    "        next_step = 'END_PHASE2_FAILURE' # Couldn't find sufficient info after retries\n",
    "    else:\n",
    "        next_step = 'retriever_agent' # Not sufficient, try retrieval again (perhaps with refined query in future phases)\n",
    "\n",
    "    return {\n",
    "        \"current_sub_query\": current_sub_query,\n",
    "        \"retrieved_chunks\": retrieved_chunks,\n",
    "        \"evaluated_sufficiency\": is_sufficient,\n",
    "        \"evaluator_feedback\": feedback,\n",
    "        \"retrieval_attempts\": retrieval_attempts,\n",
    "        \"next_agent_to_call\": next_step\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd5b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"retriever_agent\", retriever_agent_node)\n",
    "workflow.add_node(\"evaluator_agent\", evaluator_agent_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"retriever_agent\")\n",
    "\n",
    "# Define edges (transitions)\n",
    "workflow.add_edge(\"retriever_agent\", \"evaluator_agent\") # Retriever always goes to Evaluator\n",
    "\n",
    "# Conditional edge from evaluator_agent\n",
    "def decide_next_step(state: AgentState) -> Literal['retriever_agent', 'END_PHASE2', 'END_PHASE2_FAILURE']:\n",
    "    \"\"\"\n",
    "    Determines the next step based on the evaluator's decision.\n",
    "    \"\"\"\n",
    "    if state[\"evaluated_sufficiency\"]:\n",
    "        return 'END_PHASE2'\n",
    "    elif state[\"next_agent_to_call\"] == 'END_PHASE2_FAILURE':\n",
    "        return 'END_PHASE2_FAILURE' # Propagate failure state\n",
    "    else:\n",
    "        # Here, in a more advanced phase, you might have a 'query_refiner_agent'\n",
    "        # For now, it simply loops back to retriever, which will be capped by max attempts.\n",
    "        return 'retriever_agent' \n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"evaluator_agent\",\n",
    "    decide_next_step,\n",
    "    {\n",
    "        \"retriever_agent\": \"retriever_agent\",  # Loop back for another retrieval attempt\n",
    "        \"END_PHASE2\": END,                     # Exit the graph if sufficient\n",
    "        \"END_PHASE2_FAILURE\": END              # Exit if attempts exhausted or critical failure\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd6890",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Running Phase 2: Core Retrieval & Evaluation Loop ---\")\n",
    "\n",
    "# Test with a sub-query\n",
    "test_sub_query = \"My QuantumFlow purifier is showing a red light on its filter status indicator. What does this mean, and what should I do\"\n",
    "# test_sub_query = \"What is the capital of France?\" # Example of an out-of-scope query\n",
    "\n",
    "initial_state: AgentState = {\n",
    "    \"current_sub_query\": test_sub_query,\n",
    "    \"retrieved_chunks\": [],\n",
    "    \"evaluated_sufficiency\": False,\n",
    "    \"evaluator_feedback\": \"\",\n",
    "    \"retrieval_attempts\": 0,\n",
    "    \"next_agent_to_call\": 'retriever_agent' # Initial call\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "# We iterate through the states to see the progression\n",
    "final_state = None\n",
    "for s in app.stream(initial_state):\n",
    "    print(f\"\\nCurrent State after node execution: {s}\")\n",
    "    final_state = s\n",
    "\n",
    "print(\"\\n--- Phase 2 Execution Complete ---\")\n",
    "if final_state:\n",
    "    # Get the actual last state dict (s is a dict of {node_name: state_update})\n",
    "    # We need to find the final state by combining all updates\n",
    "    # LangGraph typically returns the accumulated state at each step.\n",
    "    # So, the last 's' in the loop will contain the full final state of the graph's memory.\n",
    "    \n",
    "    # Let's verify the final_state which would be the last item from the stream\n",
    "    last_node_name = list(final_state.keys())[-1]\n",
    "    final_state_values = final_state[last_node_name]\n",
    "\n",
    "\n",
    "    print(f\"\\nFinal State Summary for Query: '{final_state_values['current_sub_query']}'\")\n",
    "    print(f\"  Evaluated Sufficient: {final_state_values['evaluated_sufficiency']}\")\n",
    "    print(f\"  Retrieval Attempts: {final_state_values['retrieval_attempts']}\")\n",
    "    print(f\"  Evaluator Feedback: '{final_state_values['evaluator_feedback']}'\")\n",
    "    \n",
    "    if final_state_values['evaluated_sufficiency']:\n",
    "        print(\"  Retrieved Chunks (Content Preview):\")\n",
    "        for i, chunk in enumerate(final_state_values['retrieved_chunks']):\n",
    "            print(f\"    Chunk {i+1} (Source: {chunk.metadata.get('source', 'N/A')}): {chunk.page_content}...\")\n",
    "    else:\n",
    "        print(\"  Could not find sufficient information for this query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b1579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76a503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
