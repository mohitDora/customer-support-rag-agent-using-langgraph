{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de8ea20",
   "metadata": {},
   "source": [
    "### Import statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Literal, TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29319a8",
   "metadata": {},
   "source": [
    "### Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e088e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_DB_DIR = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"customer_support_knowledge\"\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d2661",
   "metadata": {},
   "source": [
    "### Initializing LLM and embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab181b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\", temperature=0.2, google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938d9e9",
   "metadata": {},
   "source": [
    "### Defining agent state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91815f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our RAG agent's overall workflow.\n",
    "    \"\"\"\n",
    "\n",
    "    original_query: str\n",
    "    sub_queries_list: List[str]\n",
    "    current_sub_query_index: int\n",
    "    current_sub_query: str  # The specific sub-query being worked on now\n",
    "    retrieved_chunks: List[Document]\n",
    "    evaluated_sufficiency: bool\n",
    "    evaluator_feedback: str\n",
    "    retrieval_attempts: int\n",
    "    final_answer_draft: str\n",
    "    report_formatted: str\n",
    "    accumulated_relevant_chunks: List[\n",
    "        Document\n",
    "    ]  # Accumulates chunks from all answered sub-queries\n",
    "    unanswerable_sub_queries: List[\n",
    "        str\n",
    "    ]  # To track sub-queries that couldn't be answered\n",
    "    next_agent_to_call: Literal[\n",
    "        \"research_agent\",\n",
    "        \"retriever_agent\",\n",
    "        \"evaluator_agent\",\n",
    "        \"formatter_agent\",\n",
    "        \"synthesizer_agent\",  # Will be used in Phase 4\n",
    "        \"END\",  # Final completion\n",
    "        \"FATAL_ERROR\",  # For unrecoverable errors\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e60b9",
   "metadata": {},
   "source": [
    "### Load Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_db():\n",
    "    \"\"\"Helper function to load the ChromaDB instance.\"\"\"\n",
    "    return Chroma(\n",
    "        persist_directory=CHROMA_DB_DIR,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b70740",
   "metadata": {},
   "source": [
    "### Research agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Node responsible for breaking down the original query into sub-queries\n",
    "    and managing the flow of processing them.\n",
    "    \"\"\"\n",
    "    print(\"---RESEARCH AGENT: Managing research plan---\")\n",
    "\n",
    "    original_query = state[\"original_query\"]\n",
    "    sub_queries_list = state.get(\"sub_queries_list\", [])\n",
    "    current_sub_query_index = state.get(\"current_sub_query_index\", 0)\n",
    "\n",
    "    # Check if all sub-queries are processed\n",
    "    if current_sub_query_index >= len(sub_queries_list) and len(sub_queries_list) > 0:\n",
    "        print(\"---RESEARCH AGENT: All sub-queries processed. Moving to synthesis.---\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"next_agent_to_call\": \"synthesizer_agent\",\n",
    "        }  # Transition to synthesizer in Phase 4\n",
    "\n",
    "    # Initial breakdown of query if not already done\n",
    "    if not sub_queries_list:\n",
    "        print(f\"---RESEARCH AGENT: Breaking down original query: '{original_query}'---\")\n",
    "        breakdown_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"You are a research planner. Your task is to break down a complex user query into 3-5 concise, distinct, and answerable sub-questions.\n",
    "             Each sub-question should be focused enough to be answered by retrieving information.\n",
    "             Respond with a comma-separated list of sub-questions ONLY. Do not add any other text or numbering.\n",
    "             Example: \"What are the common symptoms of flu?, How is flu transmitted?, What are flu prevention methods?\"\n",
    "             \"\"\",\n",
    "                ),\n",
    "                (\"human\", f\"Break down the query: '{original_query}'\"),\n",
    "            ]\n",
    "        )\n",
    "        breakdown_chain = breakdown_prompt | llm | StrOutputParser()\n",
    "\n",
    "        try:\n",
    "            raw_sub_queries = breakdown_chain.invoke({\"original_query\": original_query})\n",
    "            sub_queries_list = [\n",
    "                q.strip() for q in raw_sub_queries.split(\",\") if q.strip()\n",
    "            ]\n",
    "\n",
    "            if not sub_queries_list:\n",
    "                raise ValueError(\"Gemini did not return any sub-queries.\")\n",
    "\n",
    "            print(f\"---RESEARCH AGENT: Generated sub-queries: {sub_queries_list}---\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"---RESEARCH AGENT ERROR: Failed to break down query: {e}---\")\n",
    "            return {\n",
    "                **state,\n",
    "                \"next_agent_to_call\": \"FATAL_ERROR\",\n",
    "                \"evaluator_feedback\": f\"Failed to break down query: {e}\",\n",
    "            }\n",
    "\n",
    "    # Set the current sub-query to process\n",
    "    current_sub_query = sub_queries_list[current_sub_query_index]\n",
    "    print(\n",
    "        f\"---RESEARCH AGENT: Processing sub-query {current_sub_query_index + 1}/{len(sub_queries_list)}: '{current_sub_query}'---\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"sub_queries_list\": sub_queries_list,\n",
    "        \"current_sub_query_index\": current_sub_query_index,\n",
    "        \"current_sub_query\": current_sub_query,\n",
    "        \"retrieval_attempts\": 0,  # Reset attempts for new sub-query\n",
    "        \"retrieved_chunks\": [],  # Clear previous chunks\n",
    "        \"evaluated_sufficiency\": False,  # Reset evaluation\n",
    "        \"evaluator_feedback\": \"\",  # Clear previous feedback\n",
    "        \"next_agent_to_call\": \"retriever_agent\",  # Go to retrieval for this sub-query\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f65c0",
   "metadata": {},
   "source": [
    "### Retriever Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever_agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Node that retrieves relevant chunks from the vector database based on the current sub-query.\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVER AGENT: Initiating retrieval---\")\n",
    "    current_sub_query = state[\"current_sub_query\"]\n",
    "    retrieval_attempts = state.get(\"retrieval_attempts\", 0) + 1\n",
    "\n",
    "    try:\n",
    "        vector_db = get_vector_db()  # Use helper function\n",
    "        print(f\"ChromaDB loaded successfully from {CHROMA_DB_DIR}.\")\n",
    "\n",
    "        k_value = 1  # You can experiment with this number\n",
    "        retriever = vector_db.as_retriever(search_kwargs={\"k\": k_value})\n",
    "\n",
    "        print(\n",
    "            f\"Retrieving for query: '{current_sub_query}' (Attempt: {retrieval_attempts})\"\n",
    "        )\n",
    "        retrieved_chunks = retriever.invoke(current_sub_query)\n",
    "\n",
    "        print(f\"---RETRIEVER AGENT: Retrieved {len(retrieved_chunks)} chunks.---\")\n",
    "\n",
    "        return {\n",
    "            **state,  # Preserve existing state\n",
    "            \"retrieved_chunks\": retrieved_chunks,\n",
    "            \"retrieval_attempts\": retrieval_attempts,\n",
    "            \"next_agent_to_call\": \"evaluator_agent\",\n",
    "            \"evaluated_sufficiency\": False,\n",
    "            \"evaluator_feedback\": \"\",\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"---RETRIEVER AGENT ERROR: {e}---\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"retrieved_chunks\": [],\n",
    "            \"retrieval_attempts\": retrieval_attempts,\n",
    "            \"next_agent_to_call\": \"FATAL_ERROR\",  # Fatal error if retrieval setup fails\n",
    "            \"evaluator_feedback\": f\"Retrieval failed: {e}\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3da1a",
   "metadata": {},
   "source": [
    "### Evaluator agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1373938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Node that uses Gemini to evaluate if the retrieved chunks are sufficient to answer the sub-query.\n",
    "    Provides feedback if not.\n",
    "    \"\"\"\n",
    "    print(\"---EVALUATOR AGENT: Evaluating retrieved chunks---\")\n",
    "    current_sub_query = state[\"current_sub_query\"]\n",
    "    retrieved_chunks = state[\"retrieved_chunks\"]\n",
    "    retrieval_attempts = state[\"retrieval_attempts\"]\n",
    "    accumulated_relevant_chunks = state.get(\"accumulated_relevant_chunks\", [])\n",
    "    unanswerable_sub_queries = state.get(\"unanswerable_sub_queries\", [])\n",
    "    current_sub_query_index = state.get(\"current_sub_query_index\", 0)\n",
    "\n",
    "    MAX_RETRIEVAL_ATTEMPTS = 1  # Define maximum attempts for a single sub-query\n",
    "\n",
    "    if not retrieved_chunks:\n",
    "        print(\"---EVALUATOR AGENT: No chunks retrieved.---\")\n",
    "        if retrieval_attempts >= MAX_RETRIEVAL_ATTEMPTS:\n",
    "            print(\n",
    "                f\"---EVALUATOR AGENT: Max attempts reached for '{current_sub_query}'. Marking as unanswerable.---\"\n",
    "            )\n",
    "            unanswerable_sub_queries.append(current_sub_query)\n",
    "            return {\n",
    "                **state,\n",
    "                \"evaluated_sufficiency\": False,\n",
    "                \"evaluator_feedback\": \"Max retrieval attempts reached; no relevant documents found.\",\n",
    "                \"unanswerable_sub_queries\": unanswerable_sub_queries,\n",
    "                \"current_sub_query_index\": current_sub_query_index\n",
    "                + 1,  # Move to next sub-query\n",
    "                \"next_agent_to_call\": \"research_agent\",  # Let research agent determine next sub-query or conclude\n",
    "            }\n",
    "        else:\n",
    "            print(\"---EVALUATOR AGENT: No chunks retrieved, retrying retrieval.---\")\n",
    "            return {\n",
    "                **state,\n",
    "                \"evaluated_sufficiency\": False,\n",
    "                \"evaluator_feedback\": \"No relevant documents were retrieved. Reattempting retrieval.\",\n",
    "                \"next_agent_to_call\": \"retriever_agent\",  # Loop back to retriever\n",
    "            }\n",
    "\n",
    "    # Format chunks for the LLM prompt\n",
    "    formatted_chunks = \"\\n---\\n\".join([doc.page_content for doc in retrieved_chunks])\n",
    "\n",
    "    # Define the evaluation prompt for Gemini\n",
    "    eval_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are an expert evaluator for a RAG system. Your task is to determine if the provided 'CONTEXT' is sufficient and relevant to fully and comprehensively answer the 'SUB-QUERY'.\n",
    "\n",
    "         If the CONTEXT is sufficient and relevant, respond ONLY with the word 'YES'.\n",
    "         If the CONTEXT is NOT sufficient or relevant, respond ONLY with the word 'NO', followed by a concise, specific suggestion on how to improve the retrieval for this sub-query. For example:\n",
    "         - 'NO: The results are too general; focus on error codes.'\n",
    "         - 'NO: No specific instructions found for this type of issue.'\n",
    "         - 'NO: The context mentions the topic but lacks actionable steps.'\n",
    "\n",
    "         Aim for direct, actionable feedback. Do not elaborate beyond 'NO: [feedback]'.\n",
    "         \"\"\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                f\"SUB-QUERY: {current_sub_query}\\n\\nCONTEXT:\\n{formatted_chunks}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    eval_chain = eval_prompt | llm | StrOutputParser()\n",
    "\n",
    "    print(\n",
    "        f\"---EVALUATOR AGENT: Sending evaluation request to Gemini for query '{current_sub_query}'---\"\n",
    "    )\n",
    "    gemini_response = eval_chain.invoke(\n",
    "        {\"current_sub_query\": current_sub_query, \"formatted_chunks\": formatted_chunks}\n",
    "    )\n",
    "\n",
    "    gemini_response = gemini_response.strip()\n",
    "    print(f\"---EVALUATOR AGENT: Gemini's raw response: {gemini_response}---\")\n",
    "\n",
    "    is_sufficient = gemini_response.upper().startswith(\"YES\")\n",
    "    feedback = \"\"\n",
    "    if (\n",
    "        not is_sufficient and len(gemini_response) > 3\n",
    "    ):  # \"NO\" is 2 chars, so look for more\n",
    "        feedback = gemini_response[3:].strip()  # Remove \"NO:\" prefix\n",
    "\n",
    "    print(\n",
    "        f\"---EVALUATOR AGENT: Sufficiency: {is_sufficient}, Feedback: '{feedback}'---\"\n",
    "    )\n",
    "\n",
    "    next_step: Literal[\"retriever_agent\", \"research_agent\", \"FATAL_ERROR\"]\n",
    "\n",
    "    if is_sufficient:\n",
    "        # Add chunks to accumulated results\n",
    "        accumulated_relevant_chunks.extend(retrieved_chunks)\n",
    "        print(\n",
    "            f\"---EVALUATOR AGENT: Chunks deemed sufficient. Accumulated {len(retrieved_chunks)} new chunks. Total accumulated: {len(accumulated_relevant_chunks)}---\"\n",
    "        )\n",
    "        next_step = (\n",
    "            \"research_agent\"  # Move to next sub-query (handled by research_agent)\n",
    "        )\n",
    "        current_sub_query_index += 1  # Increment for next sub-query\n",
    "    elif retrieval_attempts >= MAX_RETRIEVAL_ATTEMPTS:\n",
    "        print(\n",
    "            f\"---EVALUATOR AGENT: Max retrieval attempts ({MAX_RETRIEVAL_ATTEMPTS}) reached for '{current_sub_query}'. Marking as unanswerable.---\"\n",
    "        )\n",
    "        unanswerable_sub_queries.append(current_sub_query)\n",
    "        next_step = (\n",
    "            \"research_agent\"  # Move to next sub-query (handled by research_agent)\n",
    "        )\n",
    "        current_sub_query_index += 1  # Increment for next sub-query\n",
    "    else:\n",
    "        next_step = \"retriever_agent\"  # Not sufficient, try retrieval again\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"evaluated_sufficiency\": is_sufficient,\n",
    "        \"evaluator_feedback\": feedback,\n",
    "        \"accumulated_relevant_chunks\": accumulated_relevant_chunks,\n",
    "        \"unanswerable_sub_queries\": unanswerable_sub_queries,\n",
    "        \"current_sub_query_index\": current_sub_query_index,  # Update index if moving on\n",
    "        \"next_agent_to_call\": next_step,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d81fe",
   "metadata": {},
   "source": [
    "### Synthesizer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff02dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesizer_agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Node that synthesizes all accumulated relevant chunks into a comprehensive answer\n",
    "    for the original query.\n",
    "    \"\"\"\n",
    "    print(\"---SYNTHESIZER AGENT: Generating final answer draft---\")\n",
    "    original_query = state[\"original_query\"]\n",
    "    accumulated_chunks = state[\"accumulated_relevant_chunks\"]\n",
    "    unanswerable_sub_queries = state[\"unanswerable_sub_queries\"]\n",
    "\n",
    "    if not accumulated_chunks:\n",
    "        final_answer_draft = \"I could not find sufficient information in my knowledge base to answer your query. \"\n",
    "        if unanswerable_sub_queries:\n",
    "            final_answer_draft += f\"Specifically, I couldn't find answers for: {', '.join(unanswerable_sub_queries)}. \"\n",
    "        final_answer_draft += (\n",
    "            \"Please ensure the information is available in the provided documents.\"\n",
    "        )\n",
    "        print(\"---SYNTHESIZER AGENT: No chunks accumulated, generating apology.---\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"final_answer_draft\": final_answer_draft,\n",
    "            \"next_agent_to_call\": \"formatter_agent\",\n",
    "        }\n",
    "\n",
    "    # Combine all relevant content\n",
    "    combined_content = \"\\n\\n\".join([doc.page_content for doc in accumulated_chunks])\n",
    "\n",
    "    # Craft the synthesis prompt for Gemini\n",
    "    synthesis_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are a helpful customer support AI. Your task is to synthesize the provided 'CONTEXT' to answer the 'ORIGINAL_QUERY' comprehensively and clearly.\n",
    "         \n",
    "         Only use information directly present in the CONTEXT. Do not make up information.\n",
    "         Structure your answer professionally, starting with a direct response, potentially using bullet points for steps or lists, and concluding politely.\n",
    "         \n",
    "         If any parts of the ORIGINAL_QUERY could not be addressed by the CONTEXT (and are listed as 'UNANSWERABLE_SUB_QUERIES'), acknowledge this gracefully.\n",
    "\n",
    "         Example of good structure:\n",
    "         \"Thank you for your question about [Topic]. Based on the information I have, here is the answer:\n",
    "         [Direct answer and details, using bullet points for steps]\n",
    "         \n",
    "         Regarding [unanswerable part], I was unable to find specific details in my knowledge base. Please check the latest manual or contact support for further assistance.\n",
    "         \n",
    "         I hope this helps!\"\n",
    "         \"\"\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                f\"ORIGINAL_QUERY: {original_query}\\n\\nCONTEXT:\\n{combined_content}\\n\\nUNANSWERABLE_SUB_QUERIES: {', '.join(unanswerable_sub_queries) if unanswerable_sub_queries else 'None'}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    synthesis_chain = synthesis_prompt | llm | StrOutputParser()\n",
    "\n",
    "    print(\n",
    "        f\"---SYNTHESIZER AGENT: Sending synthesis request to Gemini for query '{original_query}'---\"\n",
    "    )\n",
    "    final_answer_draft = synthesis_chain.invoke(\n",
    "        {\n",
    "            \"original_query\": original_query,\n",
    "            \"combined_content\": combined_content,\n",
    "            \"unanswerable_sub_queries\": unanswerable_sub_queries,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"---SYNTHESIZER AGENT: Draft generated.---\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"final_answer_draft\": final_answer_draft,\n",
    "        \"next_agent_to_call\": \"formatter_agent\",  # Move to formatter for polish\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63ca23",
   "metadata": {},
   "source": [
    "### Formatter agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatter_agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Node that refines and formats the final answer draft for clarity and presentation.\n",
    "    \"\"\"\n",
    "    print(\"---FORMATTER AGENT: Polishing final answer---\")\n",
    "    original_query = state[\"original_query\"]\n",
    "    final_answer_draft = state[\"final_answer_draft\"]\n",
    "\n",
    "    if not final_answer_draft:\n",
    "        print(\"---FORMATTER AGENT: No draft to format.---\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"report_formatted\": \"An error occurred during answer generation.\",\n",
    "            \"next_agent_to_call\": \"END\",\n",
    "        }\n",
    "\n",
    "    format_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are a polite and professional customer support assistant. Your task is to take a raw answer draft and refine it for clarity, grammar, and tone.\n",
    "         Ensure it is easy to read, uses appropriate formatting (like bolding keywords, clear paragraphs, bullet points for steps), and maintains a helpful, empathetic tone.\n",
    "         Do not add new information or remove critical details. Simply rephrase and format.\n",
    "         Ensure a polite opening and closing.\n",
    "         \"\"\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                f\"Refine the following answer draft for the query '{original_query}':\\n\\n{final_answer_draft}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    format_chain = format_prompt | llm | StrOutputParser()\n",
    "\n",
    "    print(\"---FORMATTER AGENT: Sending formatting request to Gemini.---\")\n",
    "    report_formatted = format_chain.invoke(\n",
    "        {\"original_query\": original_query, \"final_answer_draft\": final_answer_draft}\n",
    "    )\n",
    "\n",
    "    print(\"---FORMATTER AGENT: Answer formatted.---\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"report_formatted\": report_formatted,\n",
    "        \"next_agent_to_call\": \"END\",  # All done!\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b382d8",
   "metadata": {},
   "source": [
    "### Supervisor agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_node(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    The supervisor node orchestrates the flow based on the 'next_agent_to_call' in the state.\n",
    "    It also checks for overall completion of research.\n",
    "    \"\"\"\n",
    "    print(f\"\\n---SUPERVISOR: Directing flow to: {state['next_agent_to_call']}---\")\n",
    "\n",
    "    # If an agent signals END or FATAL_ERROR, the supervisor transitions to the graph END\n",
    "    if state[\"next_agent_to_call\"] in [\"END\", \"FATAL_ERROR\"]:\n",
    "        print(\n",
    "            \"---SUPERVISOR: Workflow complete or fatal error detected. Ending workflow.---\"\n",
    "        )\n",
    "        return {\"next_agent_to_call\": \"end_workflow\"}  # Transition to END\n",
    "\n",
    "    # Otherwise, direct to the agent specified in next_agent_to_call\n",
    "    return {\"next_agent_to_call\": state[\"next_agent_to_call\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6d90f",
   "metadata": {},
   "source": [
    "### Defining workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd5b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "workflow.add_node(\"research_agent\", research_agent_node)\n",
    "workflow.add_node(\"retriever_agent\", retriever_agent_node)\n",
    "workflow.add_node(\"evaluator_agent\", evaluator_agent_node)\n",
    "workflow.add_node(\"synthesizer_agent\", synthesizer_agent_node)  # NEW\n",
    "workflow.add_node(\"formatter_agent\", formatter_agent_node)  # NEW\n",
    "\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "# Add edges\n",
    "# All agent nodes transition back to the supervisor\n",
    "workflow.add_edge(\"research_agent\", \"supervisor\")\n",
    "workflow.add_edge(\"retriever_agent\", \"supervisor\")\n",
    "workflow.add_edge(\"evaluator_agent\", \"supervisor\")\n",
    "workflow.add_edge(\"synthesizer_agent\", \"supervisor\")  # NEW\n",
    "workflow.add_edge(\"formatter_agent\", \"supervisor\")  # NEW\n",
    "\n",
    "\n",
    "# Define conditional transitions from the supervisor\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda state: state[\n",
    "        \"next_agent_to_call\"\n",
    "    ],  # The supervisor's decision is in this state variable\n",
    "    {\n",
    "        \"research_agent\": \"research_agent\",\n",
    "        \"retriever_agent\": \"retriever_agent\",\n",
    "        \"evaluator_agent\": \"evaluator_agent\",\n",
    "        \"synthesizer_agent\": \"synthesizer_agent\",  # NEW routing for synthesizer\n",
    "        \"formatter_agent\": \"formatter_agent\",  # NEW routing for formatter\n",
    "        \"END\": END,\n",
    "        \"FATAL_ERROR\": END,  # End workflow on fatal error\n",
    "        \"end_workflow\": END,  # Explicit end if supervisor decided to end\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabc982",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Running Phase 4: Synthesis & Refinement ---\")\n",
    "\n",
    "# Test with a complex original query\n",
    "test_original_query = \"My QuantumFlow purifier is showing a red light on its filter status indicator. What does this mean, and what should I do\"\n",
    "# test_original_query = \"What is the history of the internet?\" # Example out-of-scope/no data\n",
    "\n",
    "initial_state: AgentState = {\n",
    "    \"original_query\": test_original_query,\n",
    "    \"sub_queries_list\": [],\n",
    "    \"current_sub_query_index\": 0,\n",
    "    \"current_sub_query\": \"\",\n",
    "    \"retrieved_chunks\": [],\n",
    "    \"evaluated_sufficiency\": False,\n",
    "    \"evaluator_feedback\": \"\",\n",
    "    \"retrieval_attempts\": 0,\n",
    "    \"accumulated_relevant_chunks\": [],\n",
    "    \"unanswerable_sub_queries\": [],\n",
    "    \"final_answer_draft\": \"\",\n",
    "    \"report_formatted\": \"\",\n",
    "    \"next_agent_to_call\": \"research_agent\",  # Start by asking research agent to break down query\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "final_state_summary = initial_state.copy()  # Start with a copy of initial state\n",
    "\n",
    "for s in app.stream(initial_state, config={\"recursion_limit\": 200}):\n",
    "    # Update the state summary with the latest changes from each node\n",
    "    for _key, value in s.items():\n",
    "        if isinstance(\n",
    "            value, dict\n",
    "        ):  # LangGraph updates often come as dicts for each node\n",
    "            final_state_summary.update(value)  # Merge updates\n",
    "\n",
    "    last_node_executed = list(s.keys())[-1]\n",
    "    print(\n",
    "        f\"\\nState after '{last_node_executed}': next_agent_to_call={final_state_summary.get('next_agent_to_call', 'N/A')}\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"\\n--- Phase 4 Execution Complete ---\")\n",
    "if final_state_summary:\n",
    "    print(\n",
    "        f\"\\nFinal Answer for Original Query: '{final_state_summary['original_query']}'\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Total Sub-Queries Generated: {len(final_state_summary['sub_queries_list'])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Accumulated Relevant Chunks: {len(final_state_summary['accumulated_relevant_chunks'])} chunks\"\n",
    "    )\n",
    "    if final_state_summary[\"unanswerable_sub_queries\"]:\n",
    "        print(\n",
    "            f\"  Unanswerable Sub-Queries: {final_state_summary['unanswerable_sub_queries']}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n--- GENERATED ANSWER ---\")\n",
    "    print(final_state_summary[\"report_formatted\"])\n",
    "    print(\"\\n------------------------\")\n",
    "\n",
    "    print(f\"\\nFinal Next Agent: {final_state_summary['next_agent_to_call']}\")\n",
    "else:\n",
    "    print(\"No final state accumulated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
